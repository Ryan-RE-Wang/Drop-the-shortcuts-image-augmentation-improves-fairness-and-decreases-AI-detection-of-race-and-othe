{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominican-wesley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 20 14:25:55 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.67       Driver Version: 460.67       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:65:00.0 Off |                  N/A |\r\n",
      "| 97%   84C    P2   222W / 370W |  21323MiB / 24265MiB |    100%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 3090    Off  | 00000000:B3:00.0 Off |                  N/A |\r\n",
      "|100%   81C    P2   334W / 370W |  24080MiB / 24268MiB |     92%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1716      C   /opt/tljh/user/bin/python        1891MiB |\r\n",
      "|    0   N/A  N/A      1741      G   /usr/lib/xorg/Xorg                 32MiB |\r\n",
      "|    0   N/A  N/A      3270      G   /usr/bin/gnome-shell               98MiB |\r\n",
      "|    0   N/A  N/A      4934      C   /opt/tljh/user/bin/python        1891MiB |\r\n",
      "|    0   N/A  N/A      8481      G   /usr/lib/xorg/Xorg                 49MiB |\r\n",
      "|    0   N/A  N/A     11784      C   /opt/tljh/user/bin/python         413MiB |\r\n",
      "|    0   N/A  N/A     13342      C   /opt/tljh/user/bin/python        1893MiB |\r\n",
      "|    0   N/A  N/A     18102      C   /opt/tljh/user/bin/python        1891MiB |\r\n",
      "|    0   N/A  N/A     18375      C   /opt/tljh/user/bin/python        1893MiB |\r\n",
      "|    0   N/A  N/A     20838      C   /opt/tljh/user/bin/python         413MiB |\r\n",
      "|    0   N/A  N/A     21250      C   /opt/tljh/user/bin/python        1393MiB |\r\n",
      "|    0   N/A  N/A     23074      C   /opt/tljh/user/bin/python        1893MiB |\r\n",
      "|    0   N/A  N/A     25787      C   /opt/tljh/user/bin/python        1891MiB |\r\n",
      "|    0   N/A  N/A     29133      C   /opt/tljh/user/bin/python        1891MiB |\r\n",
      "|    0   N/A  N/A     31225      C   /opt/tljh/user/bin/python        1891MiB |\r\n",
      "|    0   N/A  N/A     31650      C   /opt/tljh/user/bin/python        1891MiB |\r\n",
      "|    1   N/A  N/A      1741      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    1   N/A  N/A      8481      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    1   N/A  N/A     25210      C   python                          24067MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thirty-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from evaluation import *\n",
    "from load_data import *\n",
    "from training_function import *\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices(device_type='GPU')\n",
    "tf.config.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fundamental-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2021\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certain-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_method = '_proposed'\n",
    "dataset = 'mimic'\n",
    "task = 'disease'\n",
    "\n",
    "X_train, y_train, demo_train = get_data(aug_method=aug_method, dataset=dataset, data_split='train', task=task, return_demo=True)\n",
    "X_val, y_val, demo_val = get_data(aug_method=aug_method, dataset=dataset, data_split='val', task=task, return_demo=True)\n",
    "X_test, y_test, demo = get_data(aug_method=aug_method, dataset=dataset, data_split='test', task=task, return_demo=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train, demo_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val, demo_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equal-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'ERM'\n",
    "archi = 'densenet'\n",
    "mode = ''\n",
    "    \n",
    "model_name = 'model_{archi}_{dataset}_{algo}_proposed_no_weight'.format(archi=archi, dataset=dataset, algo=algo,\n",
    "                                                                                    task=task)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model = get_model(algo, 'ERM', task, archi, model_name)\n",
    "\n",
    "# model_name = 'model_{archi}_{dataset}_{algo}_task_transfer_{task}_proposed'.format(archi=archi, dataset=dataset, algo=algo,\n",
    "#                                                                                     task=task)    \n",
    "checkpoint_filepath = 'checkpoints/'+model_name\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_filepath, max_to_keep=1, checkpoint_name=model_name)\n",
    "\n",
    "train_step(model, algo, mode, checkpoint_filepath, optimizer, manager, train_dataset, val_dataset)\n",
    "\n",
    "save_thresh(model, model_name, manager, X_val, y_val)\n",
    "\n",
    "compute_original_predictions(model, model_name, manager, X_test)\n",
    "    \n",
    "compute_aug_predictions(model, model_name, manager)\n",
    "# compute_aug_predictions_chexpert(model, model_name, manager, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "collectible-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_method = ''\n",
    "dataset = 'mimic'\n",
    "task = 'disease'\n",
    "\n",
    "X_train, y_train, demo_train = get_data(aug_method=aug_method, dataset=dataset, data_split='train', task=task, return_demo=True)\n",
    "X_val, y_val, demo_val = get_data(aug_method=aug_method, dataset=dataset, data_split='val', task=task, return_demo=True)\n",
    "X_test, y_test, demo = get_data(aug_method=aug_method, dataset=dataset, data_split='test', task=task, return_demo=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train, demo_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val, demo_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "about-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'ERM'\n",
    "archi = 'densenet'\n",
    "mode = ''\n",
    "    \n",
    "model_name = 'model_{archi}_{dataset}_{algo}_no_weight'.format(archi=archi, dataset=dataset, algo=algo,\n",
    "                                                                                    task=task)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model = get_model(algo, 'ERM', task, archi, model_name)\n",
    "\n",
    "# model_name = 'model_{archi}_{dataset}_{algo}_task_transfer_{task}'.format(archi=archi, dataset=dataset, algo=algo,\n",
    "#                                                                                     task=task)    \n",
    "checkpoint_filepath = 'checkpoints/'+model_name\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_filepath, max_to_keep=1, checkpoint_name=model_name)\n",
    "\n",
    "train_step(model, algo, mode, checkpoint_filepath, optimizer, manager, train_dataset, val_dataset)\n",
    "\n",
    "save_thresh(model, model_name, manager, X_val, y_val)\n",
    "\n",
    "compute_original_predictions(model, model_name, manager, X_test)\n",
    "    \n",
    "compute_aug_predictions(model, model_name, manager)\n",
    "# compute_aug_predictions_chexpert(model, model_name, manager, X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

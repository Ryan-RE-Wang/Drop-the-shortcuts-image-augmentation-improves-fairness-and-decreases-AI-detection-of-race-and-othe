{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from models import *\n",
    "from evaluation import *\n",
    "from load_data import *\n",
    "\n",
    "print(tf.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices(device_type='GPU')\n",
    "tf.config.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2021\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-bulgaria",
   "metadata": {},
   "source": [
    "# Compute disparity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-blame",
   "metadata": {},
   "source": [
    "## 3D MRI disparity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_new.csv')\n",
    "data_path = '../../../mnt/usb/kuopc/ADNI_B1/MPR__GradWarp__B1_Correction_crop/'\n",
    "\n",
    "df = df.loc[df['Group'] != 'MCI']\n",
    "df = df.loc[df['Split'] == 'test']\n",
    "\n",
    "df['Group'] = df['Group'].replace(['CN', 'AD'], [0, 1])\n",
    "df['Sex'] = df['Sex'].replace(['F', 'M'], [0, 1])\n",
    "df['Age'] = np.where(df['Age'] <= 75, 0, 1)\n",
    "df['Race'] = np.where(df['Race'] < 1, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-prayer",
   "metadata": {},
   "source": [
    "### Generate disparities csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['AUC', 'BCE', 'ECE', \"Error rate\", \"Precision\"]\n",
    "group = 'race'\n",
    "testdata = 'original'\n",
    "group_type = {'race': [0, 1], 'gender': [0, 1], 'age': [0, 1]}\n",
    "group_name = {'race': ['white', 'others'], 'gender': ['Female', 'Male'], 'age': ['0-75', '75+']}\n",
    "\n",
    "result_df = pd.DataFrame(columns=metrics)\n",
    "\n",
    "for model_name in ['', '_balanced', '_stratified', '_Adv', '_DistMatchMMD', '_DistMatchMean', '_FairALM']:\n",
    "    \n",
    "    results_list = [\n",
    "                'results/3D_CNN_AD_CN{model_name}_on_original_{group}_results'.format(model_name=model_name, group=group),\n",
    "                'results/3D_CNN_AD_CN{model_name}_on_aug_{group}_results'.format(model_name=model_name, group=group),\n",
    "                'results/3D_CNN_AD_CN{model_name}_proposed_on_original_{group}_results'.format(model_name=model_name, group=group),\n",
    "                'results/3D_CNN_AD_CN{model_name}_proposed_on_aug_{group}_results'.format(model_name=model_name, group=group),\n",
    "    ]\n",
    "          \n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "\n",
    "        all_mean_disparity = []\n",
    "        all_mean_score = []\n",
    "        for idx, result_name in enumerate(results_list):\n",
    "            \n",
    "            result_df_dict = pd.DataFrame({\"AUC\":'', \"BCE\":'', \"ECE\":'', \"Error rate\":'', \"Precision\":''}, index=[result_name])\n",
    "\n",
    "            with open(\"{result_name}\".format(result_name=result_name), \"rb\") as fp:   # Unpickling\n",
    "                dfs = pickle.load(fp)\n",
    "            fp.close()\n",
    "\n",
    "            all_disparity = []\n",
    "            all_mean = []\n",
    "            mean_scores = []\n",
    "            for k in range(len(group_type[group])):\n",
    "                dfs[k].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                dfs[k].fillna(0, inplace=True)\n",
    "                mean_scores.append(dfs[k][metric].mean(skipna=True))\n",
    "\n",
    "            median = np.nanmedian(mean_scores)\n",
    "\n",
    "            disparity = 0\n",
    "            for k in range(len(group_type[group])):\n",
    "                disparity += (np.abs(mean_scores[k]-median))\n",
    "\n",
    "            all_disparity.append(disparity)\n",
    "            all_mean.append(np.nanmean(mean_scores))\n",
    "\n",
    "            all_mean_score = np.nanmean(all_mean)\n",
    "            all_mean_disparity = np.nanmean(all_disparity)\n",
    "            std_dev = np.nanstd(all_disparity)\n",
    "            std_error = std_dev / np.math.sqrt(1)\n",
    "            ci =  2.262 * std_error\n",
    "            all_lower = (np.nanmean(all_disparity) - ci)\n",
    "            all_upper = (np.nanmean(all_disparity) + ci)\n",
    "            \n",
    "            \n",
    "            result_df.at[result_name, metric] = \"{:.3f}\".format(all_mean_disparity)\n",
    "            \n",
    "result_df.to_csv('disparity_results/3D_all_result_{group}.csv'.format(group=group))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-fraction",
   "metadata": {},
   "source": [
    "### Plot disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def legend_without_duplicate_labels(figure):\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    legend = figure.legend(by_label.values(), by_label.keys(), loc='lower center', bbox_to_anchor=(1.3, 0.5))\n",
    "\n",
    "    for legend_handle in legend.legendHandles:\n",
    "        legend_handle.set_markersize(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['AUC', 'BCE', 'ECE', \"Error rate\", \"Precision\"]\n",
    "groups = ['gender', 'age']\n",
    "testdata = 'aug'\n",
    "group_type = {'race': [0, 1], 'gender': [0, 1], 'age': [0, 1]}\n",
    "\n",
    "results_list = ['results/3D_CNN_AD_CN_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_balanced_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_stratified_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_Adv_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_DistMatchMMD_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_DistMatchMean_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_FairALM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "model_list = ['Baseline', \n",
    "              'Balanced', 'Stratified', 'Adversarial learning',\n",
    "              'DistMatchMMD', \n",
    "              'DistMatchMean', 'FairALM', \n",
    "              'Proposed augmentation']\n",
    "\n",
    "color_list = ['C0','C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'black']\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(14, 7), dpi=500)\n",
    "\n",
    "for num, ax in enumerate(fig.axes):\n",
    "    \n",
    "    group = groups[num]\n",
    "\n",
    "    results_list = ['results/3D_CNN_AD_CN_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/3D_CNN_AD_CN_balanced_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/3D_CNN_AD_CN_stratified_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/3D_CNN_AD_CN_Adv_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/3D_CNN_AD_CN_DistMatchMMD_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/3D_CNN_AD_CN_DistMatchMean_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/3D_CNN_AD_CN_FairALM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/3D_CNN_AD_CN_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "\n",
    "    model_list = ['Baseline', \n",
    "                  'Balanced', 'Stratified', 'Adversarial learning',\n",
    "                  'DistMatchMMD', \n",
    "                  'DistMatchMean', 'FairALM', \n",
    "                  'Proposed augmentation']\n",
    "\n",
    "    color_list = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'black']\n",
    "\n",
    "\n",
    "\n",
    "    target_label = [0, 1, 2, 3, 4, 7, 8, 9, 11, 12]\n",
    "\n",
    "    gap_result = 5\n",
    "    gap_between_metrics = 20\n",
    "    gap_metrics = gap_result*(len(results_list)-1) + gap_between_metrics\n",
    "\n",
    "    top = int(gap_metrics*(len(metrics)-1) + gap_result*(len(results_list)/2))\n",
    "\n",
    "    ax.set_title('{group} disparity'.format(testdata=testdata, group=group.capitalize()), fontsize=13)\n",
    "    ax.set_yticks([(i*gap_metrics) for i in range(len(metrics))][::-1], metrics, fontsize=15)\n",
    "\n",
    "    plt_dot = []\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "\n",
    "        all_mean_disparity = []\n",
    "        all_mean_score = []\n",
    "        for idx, result_name in enumerate(results_list):\n",
    "\n",
    "            with open(\"{result_name}\".format(result_name=result_name), \"rb\") as fp:   # Unpickling\n",
    "                dfs = pickle.load(fp)\n",
    "\n",
    "            all_disparity = []\n",
    "            all_mean = []\n",
    "            mean_scores = []\n",
    "            for k in range(len(group_type[group])):\n",
    "                dfs[k].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                mean_scores.append(dfs[k][metric].mean(skipna=True))\n",
    "\n",
    "            median = np.nanmedian(mean_scores)\n",
    "\n",
    "            disparity = 0\n",
    "            for k in range(len(group_type[group])):\n",
    "                disparity += (np.abs(mean_scores[k]-median))\n",
    "\n",
    "            all_disparity.append(disparity)\n",
    "            all_mean.append(np.nanmean(mean_scores))\n",
    "\n",
    "            all_mean_score.append(np.nanmean(all_mean))\n",
    "            all_mean_disparity.append(np.nanmean(all_disparity))\n",
    "            std_dev = np.nanstd(all_disparity)\n",
    "            std_error = std_dev / np.math.sqrt(1)\n",
    "            ci =  2.262 * std_error\n",
    "            all_lower = (np.nanmean(all_disparity) - ci)\n",
    "            all_upper = (np.nanmean(all_disparity) + ci)\n",
    "\n",
    "            color = color_list[idx]\n",
    "\n",
    "#             ax.plot([all_upper, all_lower], [top-(i*gap_metrics+idx*gap_result), top-(i*gap_metrics+idx*gap_result)], color=color, linewidth = 0.8, label=model_list[idx])\n",
    "\n",
    "        for idx, result_name in enumerate(results_list):\n",
    "\n",
    "            color = color_list[idx]\n",
    "\n",
    "            ax.plot(all_mean_disparity[idx], top-(i*gap_metrics+idx*gap_result), 'o', color=color, markersize=3, label=model_list[idx])\n",
    "\n",
    "        ax.plot([all_mean_disparity[-1], all_mean_disparity[-1]], [top-(i*gap_metrics+idx*gap_result), top-(i*gap_metrics)], linestyle='--', color='black', linewidth = 0.5)\n",
    "\n",
    "    for i in range(len(metrics)-1):\n",
    "        ax.axhline(top-((i+1)*gap_metrics)+int(gap_between_metrics/2), linestyle='--', color='black', linewidth = 0.3)\n",
    "\n",
    "    legend_without_duplicate_labels(plt)\n",
    "    \n",
    "plt.savefig('results_imgs/3Ddisparity_on_{testdata}.jpg'.format(testdata=testdata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-spotlight",
   "metadata": {},
   "source": [
    "### Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-tobago",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "color_list = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'black']\n",
    "\n",
    "metrics = ['AUC', 'BCE', 'ECE', \"Error rate\", \"Precision\"]\n",
    "testdata = 'original'\n",
    "group_type = {'race': [0, 1], 'gender': [0, 1], 'age': [0, 1]}\n",
    "# group_name = {'race': ['white', 'others'], 'gender': ['Female', 'Male'], 'age': ['Young', 'Old']}\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, sharey=True, sharex='col', figsize=(13, 11), dpi=400)\n",
    "groups = ['age', 'gender']\n",
    "center = [(i*80-17.5) for i in range(len(results_list))][::-1]\n",
    "start = [(i*80) for i in range(len(results_list))][::-1]\n",
    "steps = [35, 35]\n",
    "\n",
    "\n",
    "model_list = ['Baseline', \n",
    "              'Balanced', 'Stratified', 'Adversarial learning',\n",
    "              'DistMatchMMD', \n",
    "              'DistMatchMean', 'FairALM', \n",
    "              'Proposed augmentation']\n",
    "\n",
    "for num, ax in enumerate(fig.axes):\n",
    "    \n",
    "    group = groups[int(num/5)]\n",
    "\n",
    "    results_list = ['results/3D_CNN_AD_CN_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_balanced_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_stratified_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_Adv_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_DistMatchMMD_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_DistMatchMean_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_FairALM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/3D_CNN_AD_CN_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "\n",
    "    metric = metrics[num%5]\n",
    "\n",
    "    if (num < 5):\n",
    "        ax.set_title(metric, fontsize=15)\n",
    "        \n",
    "    ax.set_yticks(center, model_list, fontsize=15)\n",
    "\n",
    "    for idx, result_name in enumerate(results_list):\n",
    "\n",
    "        with open(\"{result_name}\".format(result_name=result_name), \"rb\") as fp:   # Unpickling\n",
    "            dfs = pickle.load(fp)\n",
    "\n",
    "        all_mean_score = []\n",
    "        all_upper = []\n",
    "        all_lower = []\n",
    "        for k in range(len(group_type[group])):\n",
    "            \n",
    "            dfs[k].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            mean_scores = dfs[k][metric].mean(skipna=True)\n",
    "\n",
    "            all_mean_score.append(np.nanmean(mean_scores))\n",
    "            std_dev = np.nanstd(dfs[k][metric])\n",
    "            std_error = std_dev / np.math.sqrt(1)\n",
    "            ci =  2.262 * std_error\n",
    "            all_lower.append(np.nanmean(mean_scores) - ci)\n",
    "            all_upper.append(np.nanmean(mean_scores) + ci)\n",
    "\n",
    "        for k in range(len(group_type[group])):\n",
    "\n",
    "            color = color_list[k]\n",
    "            \n",
    "            if (np.isnan(all_mean_score[k])):\n",
    "                print(all_mean_score[k])\n",
    "                all_mean_score[k] = 0\n",
    "\n",
    "            ax.plot([all_upper[k], all_lower[k]], [start[idx]-k*steps[int(num/5)], start[idx]-k*steps[int(num/5)]], color=color, linewidth = 1)\n",
    "            ax.plot(all_mean_score[k], start[idx]-k*steps[int(num/5)], 'o', color=color, label=group_name[group][k], markersize=2)\n",
    "\n",
    "    for i in range(len(results_list)-1):\n",
    "        ax.axhline(start[i]-k*steps[int(num/5)]-20, linestyle='--', color='k', linewidth = 0.3)\n",
    "    \n",
    "    if (num%5==4):\n",
    "        lines = []\n",
    "        for i, g in enumerate(group_name[group]):\n",
    "            lines.append(Line2D([0], [0], color=color_list[i], label=g))\n",
    "        \n",
    "        ax.legend(handles=lines, loc='lower center', bbox_to_anchor=(1.4, 0.5))\n",
    "        \n",
    "plt.savefig('results_imgs/3Dperformance_{testdata}.jpg'.format(testdata=testdata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-sunglasses",
   "metadata": {},
   "source": [
    "## 2D CXR disparity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['AUC', 'BCE', 'ECE', \"Error rate\", \"Precision\"]\n",
    "Labels_diseases = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "testdata = 'aug'\n",
    "group_type = {'race': [0, 1, 4], 'gender': [0, 1], 'age': [0, 1, 2, 3]}\n",
    "group_name = {'race': ['White', 'Black', 'Asian'], 'gender': ['Male', 'Female'], 'age': ['0-40', '40-60', '60-80', '80+']}\n",
    "groups = ['race', 'age', 'gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-genesis",
   "metadata": {},
   "source": [
    "### Generate disparities csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'age'\n",
    "\n",
    "result_df = pd.DataFrame(columns=metrics)\n",
    "\n",
    "target_label = [0, 1, 2, 3, 4, 7, 8, 9, 11, 12]\n",
    "\n",
    "# for model_name in ['ERM', 'ERM_balanced', 'ERM_stratified', 'Adv', 'DistMatchMMD', 'DistMatchMean', 'FairALM']:\n",
    "# for model_name in 'ERM_rotation', 'ERM_shear', 'ERM_scaling', 'ERM_fisheye']:\n",
    "for model_name in ['ERM_no_weight', 'ERM_proposed_no_weight']:\n",
    "\n",
    "#     for model names are ['ERM_no_weight', 'ERM_rotation', 'ERM_shear', 'ERM_scaling', 'ERM_fisheye']\n",
    "    results_list = [\n",
    "                'results/densenet_mimic_{model_name}_on_original_{group}_results'.format(model_name=model_name, group=group),\n",
    "                'results/densenet_mimic_{model_name}_on_aug_{group}_results'.format(model_name=model_name, group=group),\n",
    "    ]\n",
    "\n",
    "\n",
    "#     for model names are ['ERM', 'ERM_balanced', 'ERM_stratified', 'Adv', 'DistMatchMMD', 'DistMatchMean', 'FairALM']   \n",
    "#     results_list = [\n",
    "#                 'results/densenet_mimic_{model_name}_on_original_{group}_results'.format(model_name=model_name, group=group),\n",
    "#                 'results/densenet_mimic_{model_name}_on_aug_{group}_results'.format(model_name=model_name, group=group),\n",
    "#                 'results/densenet_mimic_{model_name}_proposed_on_original_{group}_results'.format(model_name=model_name, group=group),\n",
    "#                 'results/densenet_mimic_{model_name}_proposed_on_aug_{group}_results'.format(model_name=model_name, group=group),\n",
    "#     ]\n",
    "\n",
    "#     for resnet results\n",
    "#     results_list = [\n",
    "#                 'results/resnet_mimic_ERM_on_original_{group}_results'.format(group=group),\n",
    "#                 'results/resnet_mimic_ERM_on_aug_{group}_results'.format(group=group),\n",
    "#                 'results/resnet_mimic_ERM_proposed_on_original_{group}_results'.format(group=group),\n",
    "#                 'results/resnet_mimic_ERM_proposed_on_aug_{group}_results'.format(group=group),\n",
    "#     ]\n",
    "\n",
    "\n",
    "#     for chexpert results\n",
    "#     results_list = [\n",
    "#             'results/densenet_Chexpert_ERM_on_original_{group}_results'.format(group=group),\n",
    "#             'results/densenet_Chexpert_ERM_on_aug_{group}_results'.format(group=group),\n",
    "#             'results/densenet_Chexpert_ERM_proposed_on_original_{group}_results'.format(group=group),\n",
    "#             'results/densenet_Chexpert_ERM_proposed_on_aug_{group}_results'.format(group=group),\n",
    "#     ]\n",
    "          \n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "\n",
    "        all_mean_disparity = []\n",
    "        all_mean_score = []\n",
    "        for idx, result_name in enumerate(results_list):\n",
    "            \n",
    "            result_df_dict = pd.DataFrame({\"AUC\":'', \"BCE\":'', \"ECE\":'', \"Error rate\":'', \"Precision\":''}, index=[result_name])\n",
    "\n",
    "            with open(\"{result_name}\".format(result_name=result_name), \"rb\") as fp:   # Unpickling\n",
    "                dfs = pickle.load(fp)\n",
    "\n",
    "            all_disparity = []\n",
    "            all_mean = []\n",
    "            for j in target_label:\n",
    "                mean_scores = []\n",
    "                for k in range(len(group_type[group])):\n",
    "                    dfs[k][j].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                    mean_scores.append(dfs[k][j][metric].mean(skipna=True))\n",
    "\n",
    "                median = np.nanmedian(mean_scores)\n",
    "\n",
    "                disparity = 0\n",
    "                for k in range(len(group_type[group])):\n",
    "                    disparity += (np.abs(mean_scores[k]-median))\n",
    "\n",
    "                all_disparity.append(disparity)\n",
    "                all_mean.append(np.nanmean(mean_scores))\n",
    "\n",
    "            all_mean_score = np.nanmean(all_mean)\n",
    "            all_mean_disparity = np.nanmean(all_disparity)\n",
    "            std_dev = np.nanstd(all_disparity)\n",
    "            std_error = std_dev / np.math.sqrt(1)\n",
    "            ci =  2.262 * std_error\n",
    "            all_lower = (np.nanmean(all_disparity) - ci)\n",
    "            all_upper = (np.nanmean(all_disparity) + ci)\n",
    "            \n",
    "            \n",
    "            result_df.at[result_name, metric] = \"{:.3f} [{:.3f} - {:.3f}]\".format(all_mean_disparity,all_lower, all_upper)\n",
    "            \n",
    "    result_df.to_csv('disparity_results/all_densenet_no_weight_mimic_{group}.csv'.format(group=group))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-magic",
   "metadata": {},
   "source": [
    "### plot disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def legend_without_duplicate_labels(figure):\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    legend = figure.legend(by_label.values(), by_label.keys(), loc='lower center', bbox_to_anchor=(1.4, 0.5))\n",
    "\n",
    "    for legend_handle in legend.legendHandles:\n",
    "        legend_handle.set_markersize(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-crown",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(14, 7), dpi=500)\n",
    "testdata = 'original'\n",
    "\n",
    "for num, ax in enumerate(fig.axes):\n",
    "    \n",
    "    group = groups[num]\n",
    "\n",
    "#     results_list = ['results/densenet_mimic_ERM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "#                     'results/densenet_mimic_ERM_balanced_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "#                     'results/densenet_mimic_ERM_stratified_on_{testdata}_{group}_results'.format(testdata=testdata, group=group), \n",
    "#                     'results/densenet_mimic_Adv_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "#                     'results/densenet_mimic_DistMatchMMD_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "#                     'results/densenet_mimic_DistMatchMean_on_{testdata}_{group}_results'.format(testdata=testdata, group=group), \n",
    "#                     'results/densenet_mimic_FairALM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "#                     'results/densenet_mimic_ERM_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "#     model_list = ['Baseline', \n",
    "#                   'Balanced', 'Stratified', 'Adversarial learning',\n",
    "#                   'DistMatchMMD', \n",
    "#                   'DistMatchMean', 'FairALM', \n",
    "#                   'Proposed augmentation']\n",
    "\n",
    "#     color_list = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'black']\n",
    "\n",
    "\n",
    "\n",
    "    results_list = ['results/resnet_mimic_ERM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/resnet_mimic_ERM_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "\n",
    "#     results_list = ['results/densenet_Chexpert_ERM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "#                     'results/densenet_Chexpert_ERM_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "    model_list = ['Baseline', \n",
    "                  'Proposed augmentation']\n",
    "\n",
    "    color_list = ['C0', 'black']\n",
    "\n",
    "\n",
    "\n",
    "    target_label = [0, 1, 2, 3, 4, 7, 8, 9, 11, 12]\n",
    "\n",
    "    gap_result = 5\n",
    "    gap_between_metrics = 20\n",
    "    gap_metrics = gap_result*(len(results_list)-1) + gap_between_metrics\n",
    "\n",
    "    top = int(gap_metrics*(len(metrics)-1) + gap_result*(len(results_list)/2))\n",
    "\n",
    "    ax.set_title('{group} disparity'.format(testdata=testdata, group=group.capitalize()), fontsize=13)\n",
    "    ax.set_yticks([(i*gap_metrics) for i in range(len(metrics))][::-1], metrics, fontsize=15)\n",
    "\n",
    "    plt_dot = []\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "\n",
    "        all_mean_disparity = []\n",
    "        all_mean_score = []\n",
    "        for idx, result_name in enumerate(results_list):\n",
    "\n",
    "            with open(\"{result_name}\".format(result_name=result_name), \"rb\") as fp:   # Unpickling\n",
    "                dfs = pickle.load(fp)\n",
    "\n",
    "            all_disparity = []\n",
    "            all_mean = []\n",
    "            for j in target_label:\n",
    "                mean_scores = []\n",
    "                for k in range(len(group_type[group])):\n",
    "                    dfs[k][j].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                    mean_scores.append(dfs[k][j][metric].mean(skipna=True))\n",
    "\n",
    "                median = np.nanmedian(mean_scores)\n",
    "\n",
    "                disparity = 0\n",
    "                for k in range(len(group_type[group])):\n",
    "                    disparity += (np.abs(mean_scores[k]-median))\n",
    "\n",
    "                all_disparity.append(disparity)\n",
    "                all_mean.append(np.nanmean(mean_scores))\n",
    "\n",
    "            all_mean_score.append(np.nanmean(all_mean))\n",
    "            all_mean_disparity.append(np.nanmean(all_disparity))\n",
    "            std_dev = np.nanstd(all_disparity)\n",
    "            std_error = std_dev / np.math.sqrt(1)\n",
    "            ci =  2.262 * std_error\n",
    "            all_lower = (np.nanmean(all_disparity) - ci)\n",
    "            all_upper = (np.nanmean(all_disparity) + ci)\n",
    "\n",
    "            color = color_list[idx]\n",
    "\n",
    "            ax.plot([all_upper, all_lower], [top-(i*gap_metrics+idx*gap_result), top-(i*gap_metrics+idx*gap_result)], color=color, linewidth = 0.8, label=model_list[idx])\n",
    "\n",
    "        for idx, result_name in enumerate(results_list):\n",
    "\n",
    "            color = color_list[idx]\n",
    "\n",
    "            ax.plot(all_mean_disparity[idx], top-(i*gap_metrics+idx*gap_result), 'o', color=color, markersize=3)\n",
    "\n",
    "        ax.plot([all_mean_disparity[-1], all_mean_disparity[-1]], [top-(i*gap_metrics+idx*gap_result), top-(i*gap_metrics)], linestyle='--', color='black', linewidth = 0.5)\n",
    "\n",
    "    for i in range(len(metrics)-1):\n",
    "        ax.axhline(top-((i+1)*gap_metrics)+int(gap_between_metrics/2), linestyle='--', color='black', linewidth = 0.3)\n",
    "\n",
    "    legend_without_duplicate_labels(plt)\n",
    "    \n",
    "plt.savefig('results_imgs/disparity_resnet_on_{testdata}.jpg'.format(testdata=testdata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-property",
   "metadata": {},
   "source": [
    "### Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-congo",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "color_list = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'black']\n",
    "\n",
    "testdata = 'original'\n",
    "\n",
    "model_list = ['Baseline', \n",
    "              'Balanced', 'Stratified', 'Adversarial learning',\n",
    "              'DistMatchMMD', \n",
    "              'DistMatchMean', 'FairALM', \n",
    "              'Proposed augmentation']\n",
    "target_label = [0, 1, 2, 3, 4, 7, 8, 9, 11, 12]\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, axs = plt.subplots(nrows=3, ncols=5, sharey=True, sharex='col', figsize=(13, 11), dpi=400)\n",
    "groups = ['race', 'age', 'gender']\n",
    "center = [(i*80-17.5) for i in range(len(model_list))][::-1]\n",
    "start = [(i*80) for i in range(len(model_list))][::-1]\n",
    "steps = [17.5, 11.67, 35]\n",
    "\n",
    "for num, ax in enumerate(fig.axes):\n",
    "    \n",
    "    group = groups[int(num/5)]\n",
    "\n",
    "    results_list = ['results/densenet_mimic_ERM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/densenet_mimic_ERM_balanced_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/densenet_mimic_ERM_stratified_on_{testdata}_{group}_results'.format(testdata=testdata, group=group), \n",
    "                    'results/densenet_mimic_Adv_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/densenet_mimic_DistMatchMMD_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/densenet_mimic_DistMatchMean_on_{testdata}_{group}_results'.format(testdata=testdata, group=group), \n",
    "                    'results/densenet_mimic_FairALM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                    'results/densenet_mimic_ERM_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "\n",
    "    metric = metrics[num%5]\n",
    "\n",
    "    if (num < 5):\n",
    "        ax.set_title(metric, fontsize=15)\n",
    "        \n",
    "    ax.set_yticks(center, model_list, fontsize=15)\n",
    "\n",
    "    for idx, result_name in enumerate(results_list):\n",
    "\n",
    "        with open(\"{result_name}\".format(result_name=result_name), \"rb\") as fp:   # Unpickling\n",
    "            dfs = pickle.load(fp)\n",
    "\n",
    "        all_mean_score = []\n",
    "        all_upper = []\n",
    "        all_lower = []\n",
    "        for k in range(len(group_type[group])):\n",
    "            mean_scores = []\n",
    "            for j in target_label:\n",
    "                dfs[k][j].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                mean_scores.append(dfs[k][j][metric].mean(skipna=True))\n",
    "\n",
    "            all_mean_score.append(np.nanmean(mean_scores))\n",
    "            std_dev = np.nanstd(mean_scores)\n",
    "            std_error = std_dev / np.math.sqrt(1)\n",
    "            ci =  2.262 * std_error\n",
    "            all_lower.append(np.nanmean(mean_scores) - ci)\n",
    "            all_upper.append(np.nanmean(mean_scores) + ci)\n",
    "\n",
    "        for k in range(len(group_type[group])):\n",
    "\n",
    "            color = color_list[k]\n",
    "\n",
    "            ax.plot([all_upper[k], all_lower[k]], [start[idx]-k*steps[int(num/5)], start[idx]-k*steps[int(num/5)]], color=color, linewidth = 1)\n",
    "            ax.plot(all_mean_score[k], start[idx]-k*steps[int(num/5)], 'o', color=color, label=group_name[group][k], markersize=2)\n",
    "\n",
    "    for i in range(len(results_list)-1):\n",
    "        ax.axhline(start[i]-k*steps[int(num/5)]-20, linestyle='--', color='k', linewidth = 0.3)\n",
    "    \n",
    "    if (num%5==4):\n",
    "        lines = []\n",
    "        for i, g in enumerate(group_name[group]):\n",
    "            lines.append(Line2D([0], [0], color=color_list[i], label=g))\n",
    "        \n",
    "        ax.legend(handles=lines, loc='lower center', bbox_to_anchor=(1.4, 0.5))\n",
    "        \n",
    "plt.savefig('results_imgs/performance_{testdata}.jpg'.format(testdata=testdata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "testdata = 'original'\n",
    "\n",
    "color_list = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'black']\n",
    "model_list = ['Baseline', 'Proposed augmentation']\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, axs = plt.subplots(nrows=3, ncols=5, sharey=True, sharex='col', figsize=(13, 11), dpi=400)\n",
    "groups = ['race', 'age', 'gender']\n",
    "\n",
    "\n",
    "start = [20, 10]\n",
    "center = [17.5, 7.5]\n",
    "steps = [2.5, 1.75, 5]\n",
    "for num, ax in enumerate(fig.axes):\n",
    "    \n",
    "    group = groups[int(num/5)]\n",
    "\n",
    "\n",
    "#     results_list = ['results/densenet_Chexpert_ERM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "#                 'results/densenet_Chexpert_ERM_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "    results_list = ['results/resnet_mimic_ERM_on_{testdata}_{group}_results'.format(testdata=testdata, group=group),\n",
    "                'results/resnet_mimic_ERM_proposed_on_{testdata}_{group}_results'.format(testdata=testdata, group=group)]\n",
    "\n",
    "    metric = metrics[num%5]\n",
    "\n",
    "    if (num < 5):\n",
    "        ax.set_title(metric, fontsize=15)\n",
    "        \n",
    "    ax.set_yticks(center, model_list, fontsize=15)\n",
    "\n",
    "    for idx, result_name in enumerate(results_list):\n",
    "\n",
    "        with open(\"{result_name}\".format(result_name=result_name), \"rb\") as fp:   # Unpickling\n",
    "            dfs = pickle.load(fp)\n",
    "\n",
    "        all_mean_score = []\n",
    "        all_upper = []\n",
    "        all_lower = []\n",
    "        for k in range(len(group_type[group])):\n",
    "            mean_scores = []\n",
    "            for j in target_label:\n",
    "                dfs[k][j].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                mean_scores.append(dfs[k][j][metric].mean(skipna=True))\n",
    "\n",
    "            all_mean_score.append(np.nanmean(mean_scores))\n",
    "            std_dev = np.nanstd(mean_scores)\n",
    "            std_error = std_dev / np.math.sqrt(1)\n",
    "            ci =  2.262 * std_error\n",
    "            all_lower.append(np.nanmean(mean_scores) - ci)\n",
    "            all_upper.append(np.nanmean(mean_scores) + ci)\n",
    "\n",
    "        for k in range(len(group_type[group])):\n",
    "\n",
    "            color = color_list[k]\n",
    "\n",
    "            ax.plot([all_upper[k], all_lower[k]], [start[idx]-k*steps[int(num/5)], start[idx]-k*steps[int(num/5)]], color=color, linewidth = 1)\n",
    "            ax.plot(all_mean_score[k], start[idx]-k*steps[int(num/5)], 'o', color=color, label=group_name[group][k], markersize=2)\n",
    "\n",
    "    for i in range(len(results_list)-1):\n",
    "        ax.axhline(start[i]-k*steps[int(num/5)]-2.5, linestyle='--', color='k', linewidth = 0.3)\n",
    "    \n",
    "    if (num%5==4):\n",
    "        lines = []\n",
    "        for i, g in enumerate(group_name[group]):\n",
    "            lines.append(Line2D([0], [0], color=color_list[i], label=g))\n",
    "        \n",
    "        ax.legend(handles=lines, loc='lower center', bbox_to_anchor=(1.4, 0.5))\n",
    "        \n",
    "plt.savefig('results_imgs/performance_resnet_{testdata}.jpg'.format(testdata=testdata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-incidence",
   "metadata": {},
   "source": [
    "## Task transfer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def legend_without_duplicate_labels(figure):\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    legend = figure.legend(by_label.values(), by_label.keys(), loc='lower center', bbox_to_anchor=(1.12, 0.5), fontsize=6)\n",
    "\n",
    "    for legend_handle in legend.legendHandles:\n",
    "        legend_handle.set_markersize(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset='mimic'):\n",
    "    \n",
    "    np.random.seed(2021)\n",
    "            \n",
    "    race_labels = []\n",
    "    age_labels = []\n",
    "    gender_labels = []\n",
    "    \n",
    "    filename = 'data/{dataset}_test.tfrecords'.format(dataset=dataset)\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "    for raw_record in raw_dataset:\n",
    "        \n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        \n",
    "        race = example.features.feature['race'].int64_list.value[0]\n",
    "        age = example.features.feature['age'].int64_list.value[0]\n",
    "        if (dataset == 'mimic' and age > 0):\n",
    "            age -= 1\n",
    "        gender = example.features.feature['gender'].int64_list.value[0]\n",
    "                        \n",
    "                        \n",
    "        if (race == 0):\n",
    "            race_labels.append([1, 0, 0])\n",
    "        elif (race == 1):\n",
    "            race_labels.append([0, 1, 0])\n",
    "        else:\n",
    "            race_labels.append([0, 0, 1])\n",
    "                \n",
    "            \n",
    "        if (age == 0):\n",
    "            age_labels.append([1, 0, 0, 0])\n",
    "        elif (age == 1):\n",
    "            age_labels.append([0, 1, 0, 0])\n",
    "        elif (age == 2):\n",
    "            age_labels.append([0, 0, 1, 0])\n",
    "        else:\n",
    "            age_labels.append([0, 0, 0, 1])\n",
    "                \n",
    "            \n",
    "        if (gender == 0):\n",
    "            gender_labels.append([1, 0])\n",
    "        else:\n",
    "            gender_labels.append([0, 1])\n",
    "                            \n",
    "    \n",
    "    return np.array(race_labels), np.array(age_labels), np.array(gender_labels)\n",
    "\n",
    "dataset = 'mimic'\n",
    "\n",
    "race_labels, age_labels, gender_labels = get_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-arbitration",
   "metadata": {},
   "source": [
    "### Plot CXR task transfer figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['C0', 'C1']\n",
    "\n",
    "archi = 'resnet'\n",
    "\n",
    "results_list = ['predictions/model_{archi}_{dataset}_ERM_task_transfer_race_on_original'.format(archi=archi, dataset=dataset),\n",
    "                'predictions/model_{archi}_{dataset}_ERM_task_transfer_race_proposed_on_aug'.format(archi=archi, dataset=dataset),\n",
    "                'predictions/model_{archi}_{dataset}_ERM_task_transfer_age_on_original'.format(archi=archi, dataset=dataset),\n",
    "                'predictions/model_{archi}_{dataset}_ERM_task_transfer_age_proposed_on_aug'.format(archi=archi, dataset=dataset),\n",
    "                'predictions/model_{archi}_{dataset}_ERM_task_transfer_gender_on_original'.format(archi=archi, dataset=dataset),\n",
    "                'predictions/model_{archi}_{dataset}_ERM_task_transfer_gender_proposed_on_aug'.format(archi=archi, dataset=dataset)\n",
    "                ]\n",
    "\n",
    "model_list = ['Race', 'Age', 'Geder']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 2), dpi = 300)\n",
    "plt.title('AUC of CXR task transfer', fontsize=8)\n",
    "plt.yticks([9, 5, 1], model_list, fontsize=8)\n",
    "plt.xticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "loc = [10, 8, 6, 4, 2, 0]\n",
    "\n",
    "\n",
    "for k in range(len(results_list)):\n",
    "                            \n",
    "    with open(results_list[k], \"rb\") as fp:\n",
    "        y_preds = pickle.load(fp)\n",
    "    fp.close()\n",
    "        \n",
    "    if (int(k/2) == 0):\n",
    "        all_mean_score, all_lower, all_upper = task_transfer_test(y_preds, race_labels)\n",
    "    elif (int(k/2) == 1):\n",
    "        all_mean_score, all_lower, all_upper = task_transfer_test(y_preds, age_labels)\n",
    "    else:\n",
    "        all_mean_score, all_lower, all_upper = task_transfer_test(y_preds, gender_labels)\n",
    "        \n",
    "    if (k % 2 == 0):\n",
    "        label = 'Baseline'\n",
    "    else:\n",
    "        label = 'Proposed'\n",
    "        \n",
    "    plt.plot([all_lower, all_upper], [loc[k], loc[k]], color=color_list[k%2])\n",
    "    plt.plot(all_mean_score, loc[k], 'o', color=color_list[k%2], label=label, markersize=3)\n",
    "    \n",
    "    print(all_mean_score, all_lower, all_upper)\n",
    "    \n",
    "    \n",
    "legend_without_duplicate_labels(plt)\n",
    "\n",
    "plt.savefig('results_imgs/{dataset}_{archi}_task_transfer_2d.jpg'.format(archi=archi, dataset=dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-portland",
   "metadata": {},
   "source": [
    "### Plot MRI task transfer figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_new.csv')\n",
    "data_path = '../../../mnt/usb/kuopc/ADNI_B1/MPR__GradWarp__B1_Correction_crop/'\n",
    "\n",
    "df = df.loc[df['Group'] != 'MCI']\n",
    "df = df.loc[df['Split'] == 'test']\n",
    "\n",
    "df['Group'] = df['Group'].replace(['CN', 'AD'], [0, 1])\n",
    "df['Sex'] = df['Sex'].replace(['F', 'M'], [0, 1])\n",
    "df['Age'] = np.where(df['Age'] <= 75, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['C0', 'C1']\n",
    "\n",
    "results_list = [\n",
    "                'predictions/3D_CNN_AD_CN_task_transfer_age_2_on_original',\n",
    "                'predictions/3D_CNN_AD_CN_proposed_task_transfer_age_2_on_original',\n",
    "                'predictions/3D_CNN_AD_CN_task_transfer_gender_on_original',\n",
    "                'predictions/3D_CNN_AD_CN_proposed_task_transfer_gender_on_original']\n",
    "\n",
    "model_list = ['Age', 'Gender']\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 2), dpi = 300)\n",
    "plt.title('AUC of brain MRI task transfer', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.yticks([5, 3], model_list, fontsize=8)\n",
    "plt.xticks(fontsize=5)\n",
    "loc = [5.5, 4.5, 3.5, 2.5]\n",
    "\n",
    "\n",
    "for k in range(len(results_list)):\n",
    "                            \n",
    "    with open(results_list[k], \"rb\") as fp:\n",
    "        y_preds = CPU_Unpickler(fp).load()\n",
    "    fp.close()\n",
    "        \n",
    "    if (k <= 1):\n",
    "        all_mean_score, all_lower, all_upper = task_transfer_test(y_preds, df['Age'].values)\n",
    "    else:\n",
    "        all_mean_score, all_lower, all_upper = task_transfer_test(y_preds, df['Sex'].values)\n",
    "\n",
    "    if (k % 2 == 0):\n",
    "        label = 'Baseline'\n",
    "    else:\n",
    "        label = 'Proposed'\n",
    "        \n",
    "    plt.plot([all_upper, all_lower], [loc[k], loc[k]], color=color_list[k%2])\n",
    "    plt.plot(all_mean_score, loc[k], 'o', color=color_list[k%2], label=label, markersize=3)\n",
    "    \n",
    "    print(all_mean_score, all_lower, all_upper)\n",
    "    \n",
    "    \n",
    "legend_without_duplicate_labels(plt)\n",
    "\n",
    "plt.savefig('results_imgs/mimic_resnet_task_transfer_3d.jpg', bbox_inches='tight', transparent=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'Error rate'\n",
    "\n",
    "target_result = 2\n",
    "\n",
    "for group in ['race', 'age', 'gender']:\n",
    "\n",
    "    df = pd.read_csv('disparity_results/all_result_densenet_{group}.csv'.format(group=group), index_col=0)\n",
    "    \n",
    "    print(df[metric].values[0], df[metric].values[target_result])\n",
    "    print(group, ':', np.round(100*(float(df[metric].values[0][:5])-float(df[metric].values[target_result][:5]))/float(df[metric].values[0][:5]), 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in ['age', 'gender']:\n",
    "\n",
    "    df = pd.read_csv('disparity_results/3D_all_result_{group}.csv'.format(group=group), index_col=0)\n",
    "    \n",
    "    print(df[metric].values[0], df[metric].values[target_result])\n",
    "    print(group, ':', np.round(100*(float(df[metric].values[0])-float(df[metric].values[target_result]))/float(df[metric].values[0]), 2), '%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_transfer_test(y_preds, y_test, best_thresh):\n",
    "    \n",
    "    n_bootstraps = 1000\n",
    "    rng_seed = 2021  # control reproducibility\n",
    "    er = []\n",
    "    auc = []\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        \n",
    "        indices = rng.randint(0, len(y_preds), len(y_preds))\n",
    "        \n",
    "        if len(np.unique(y_test[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            # to be defined: reject the sample\n",
    "            continue\n",
    "            \n",
    "        auc.append(roc_auc_score(y_test[indices], y_preds[indices]))\n",
    "        \n",
    "        y_preds_ = np.where(y_preds > best_thresh, 1, 0)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test[indices], y_preds_[indices]).ravel()\n",
    "        er.append((fp + fn) / (tn + fp + fn + tp))\n",
    "        \n",
    "    return np.nanmean(er), np.nanmean(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "ers = []\n",
    "\n",
    "_, y_test = get_data(aug_method='', dataset='mimic', data_split='test', task='race', return_demo=False, only_label=True)\n",
    "\n",
    "best_thresh = np.loadtxt('thresh/model_densenet_mimic_ERM_race_thresh.txt')\n",
    "\n",
    "with open('predictions/model_densenet_mimic_ERM_race_on_original', \"rb\") as fp:   # Unpickling\n",
    "    y_preds = pickle.load(fp)\n",
    "\n",
    "for k in range(3):\n",
    "\n",
    "    ers.append(task_transfer_test(y_preds[:, k], y_test[:, k], best_thresh[k]))\n",
    "\n",
    "print(np.mean(ers, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "ers = []\n",
    "\n",
    "_, y_test = get_data(aug_method='', dataset='mimic', data_split='test', task='race', return_demo=False, only_label=True)\n",
    "\n",
    "best_thresh = np.loadtxt('thresh/model_densenet_mimic_ERM_race_proposed_thresh.txt')\n",
    "\n",
    "with open('predictions/model_densenet_mimic_ERM_race_proposed_on_original', \"rb\") as fp:   # Unpickling\n",
    "    y_preds = pickle.load(fp)\n",
    "\n",
    "for k in range(3):\n",
    "\n",
    "    ers.append(task_transfer_test(y_preds[:, k], y_test[:, k], best_thresh[k]))\n",
    "    \n",
    "print(np.mean(ers, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresh = np.loadtxt('thresh/3D_CNN_AD_CN_thresh.txt')\n",
    "\n",
    "with open('predictions/3D_CNN_AD_CN_on_original', \"rb\") as fp:   # Unpickling\n",
    "    y_preds = pickle.load(fp)\n",
    "\n",
    "ers = task_transfer_test(y_preds, df['Group'].values, best_thresh)\n",
    "    \n",
    "print(np.mean(ers[0]), np.mean(ers[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresh = np.loadtxt('thresh/3D_CNN_AD_CN_proposed_thresh.txt')\n",
    "\n",
    "with open('predictions/3D_CNN_AD_CN_proposed_on_original', \"rb\") as fp:   # Unpickling\n",
    "    y_preds = pickle.load(fp)\n",
    "\n",
    "\n",
    "ers = task_transfer_test(y_preds, df['Group'].values, best_thresh)\n",
    "    \n",
    "print(np.mean(ers[0]), np.mean(ers[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
